{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232438397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>232029206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>232344069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27553029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3881097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         txId  class\n",
       "3   232438397      0\n",
       "9   232029206      0\n",
       "10  232344069      0\n",
       "11   27553029      0\n",
       "16    3881097      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classesDF  = pd.read_csv('data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
    "unlabled_classesDF  = pd.read_csv('data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
    "# Remove all nodes that have unknown class\n",
    "classesDF = classesDF[classesDF['class'] != \"unknown\"]\n",
    "# Get all nodes that have unknown class\n",
    "unlabled_classesDF = unlabled_classesDF[unlabled_classesDF['class'] == \"unknown\"]\n",
    "\n",
    "# If class = 2, then map to 0, else map to 1\n",
    "classesDF['class'] = classesDF['class'].map({'2': 0, '1': 1})\n",
    "unlabled_classesDF['class'] = unlabled_classesDF['class'].map({'unknown': -1})\n",
    "\n",
    "classesDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230459870</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  class\n",
       "0  230425980     -1\n",
       "1    5530458     -1\n",
       "2  232022460     -1\n",
       "4  230460314     -1\n",
       "5  230459870     -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabled_classesDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>timestep</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232438397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>232029206</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.005027</td>\n",
       "      <td>0.578941</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>4.380281</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>4.667146</td>\n",
       "      <td>0.851305</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.604120</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.333211</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>232344069</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147852</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.137933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27553029</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.141519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539735</td>\n",
       "      <td>-0.582077</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3881097</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172306</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>-0.163640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.054450</td>\n",
       "      <td>-1.760926</td>\n",
       "      <td>-1.760984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         txId  timestep        f0        f1        f2         f3        f4  \\\n",
       "2   232438397         1  0.163054  1.963790 -0.646376  12.409294 -0.063725   \n",
       "8   232029206         1 -0.005027  0.578941 -0.091383   4.380281 -0.063725   \n",
       "9   232344069         1 -0.147852 -0.184668 -1.201369  -0.121970 -0.043875   \n",
       "10   27553029         1 -0.151357 -0.184668 -1.201369  -0.121970 -0.043875   \n",
       "15    3881097         1 -0.172306 -0.184668 -1.201369   0.028105 -0.043875   \n",
       "\n",
       "          f5         f6        f7  ...      f155      f156      f157  \\\n",
       "2   9.782742  12.414558 -0.163645  ... -0.577099 -0.613614  0.241128   \n",
       "8   4.667146   0.851305 -0.163645  ... -0.577099 -0.613614  0.241128   \n",
       "9  -0.113002  -0.061584 -0.137933  ... -0.577099 -0.613614  0.241128   \n",
       "10 -0.113002  -0.061584 -0.141519  ... -0.539735 -0.582077 -0.979074   \n",
       "15 -0.029140   0.242712 -0.163640  ... -0.577099 -0.600999  0.241128   \n",
       "\n",
       "        f158      f159      f160      f161      f162      f163      f164  \n",
       "2   0.241406  1.072793  0.085530 -0.131155  0.677799 -0.120613 -0.119792  \n",
       "8   0.241406  0.604120  0.008632 -0.131155  0.333211 -0.120613 -0.119792  \n",
       "9   0.241406  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "10 -0.978556  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "15  0.241406  0.018279 -0.068266 -0.084674 -0.054450 -1.760926 -1.760984  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresDF = pd.read_csv('data/elliptic_bitcoin_dataset/elliptic_txs_features.csv')\n",
    "unlabled_featuresDF = pd.read_csv('data/elliptic_bitcoin_dataset/elliptic_txs_features.csv')\n",
    "# Change column names --> Column 1 is txId, Column 2 is timestep and the rest are unknown features\n",
    "featuresDF.columns = ['txId', 'timestep'] + ['f' + str(i) for i in range(165)]\n",
    "unlabled_featuresDF.columns = ['txId', 'timestep'] + ['f' + str(i) for i in range(165)]\n",
    "\n",
    "# Remove all edges that do not appear in classesDF\n",
    "featuresDF = featuresDF[featuresDF['txId'].isin(classesDF['txId'])]\n",
    "unlabled_featuresDF = unlabled_featuresDF[unlabled_featuresDF['txId'].isin(unlabled_classesDF['txId'])]\n",
    "\n",
    "featuresDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>timestep</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947382</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670883</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230460314</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511871</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230459870</td>\n",
       "      <td>1</td>\n",
       "      <td>0.961040</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.303743</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.480381</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504702</td>\n",
       "      <td>-0.422589</td>\n",
       "      <td>-0.226790</td>\n",
       "      <td>-0.117629</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.413931</td>\n",
       "      <td>1.149556</td>\n",
       "      <td>-0.696053</td>\n",
       "      <td>-0.695540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230333930</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171264</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.161887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569626</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  timestep        f0        f1        f2        f3        f4  \\\n",
       "0    5530458         1 -0.171484 -0.184668 -1.201369 -0.121970 -0.043875   \n",
       "1  232022460         1 -0.172107 -0.184668 -1.201369 -0.121970 -0.043875   \n",
       "3  230460314         1  1.011523 -0.081127 -1.201369  1.153668  0.333276   \n",
       "4  230459870         1  0.961040 -0.081127 -1.201369  1.303743  0.333276   \n",
       "5  230333930         1 -0.171264 -0.184668 -1.201369 -0.121970 -0.043875   \n",
       "\n",
       "         f5        f6        f7  ...      f155      f156      f157      f158  \\\n",
       "0 -0.113002 -0.061584 -0.162112  ...  0.947382  0.673103 -0.979074 -0.978556   \n",
       "1 -0.113002 -0.061584 -0.162749  ...  0.670883  0.439728 -0.979074 -0.978556   \n",
       "3  1.312656 -0.061584 -0.163523  ... -0.511871 -0.400422  0.517257  0.579382   \n",
       "4  1.480381 -0.061584 -0.163577  ... -0.504702 -0.422589 -0.226790 -0.117629   \n",
       "5 -0.113002 -0.061584 -0.161887  ... -0.569626 -0.607306 -0.979074 -0.978556   \n",
       "\n",
       "       f159      f160      f161      f162      f163      f164  \n",
       "0  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "1 -0.098889 -0.106715 -0.131155 -0.183671 -0.120613 -0.119792  \n",
       "3  0.018279  0.277775  0.326394  1.293750  0.178136  0.179117  \n",
       "4  0.018279  0.277775  0.413931  1.149556 -0.696053 -0.695540  \n",
       "5  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabled_featuresDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230459870</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203762</th>\n",
       "      <td>157581340</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203764</th>\n",
       "      <td>173077460</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203765</th>\n",
       "      <td>158577750</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203767</th>\n",
       "      <td>158654197</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203768</th>\n",
       "      <td>157597225</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             txId  class\n",
       "0       230425980     -1\n",
       "1         5530458     -1\n",
       "2       232022460     -1\n",
       "4       230460314     -1\n",
       "5       230459870     -1\n",
       "...           ...    ...\n",
       "203762  157581340     -1\n",
       "203764  173077460     -1\n",
       "203765  158577750     -1\n",
       "203767  158654197     -1\n",
       "203768  157597225     -1\n",
       "\n",
       "[157205 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabled_classesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "      <th>timestep</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>...</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232438397</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232029206</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.005027</td>\n",
       "      <td>0.578941</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>4.380281</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>4.667146</td>\n",
       "      <td>0.851305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.604120</td>\n",
       "      <td>0.008632</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.333211</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232344069</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.147852</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27553029</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539735</td>\n",
       "      <td>-0.582077</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3881097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172306</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>0.242712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.054450</td>\n",
       "      <td>-1.760926</td>\n",
       "      <td>-1.760984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  class  timestep        f0        f1        f2         f3  \\\n",
       "0  232438397      0         1  0.163054  1.963790 -0.646376  12.409294   \n",
       "1  232029206      0         1 -0.005027  0.578941 -0.091383   4.380281   \n",
       "2  232344069      0         1 -0.147852 -0.184668 -1.201369  -0.121970   \n",
       "3   27553029      0         1 -0.151357 -0.184668 -1.201369  -0.121970   \n",
       "4    3881097      0         1 -0.172306 -0.184668 -1.201369   0.028105   \n",
       "\n",
       "         f4        f5         f6  ...      f155      f156      f157      f158  \\\n",
       "0 -0.063725  9.782742  12.414558  ... -0.577099 -0.613614  0.241128  0.241406   \n",
       "1 -0.063725  4.667146   0.851305  ... -0.577099 -0.613614  0.241128  0.241406   \n",
       "2 -0.043875 -0.113002  -0.061584  ... -0.577099 -0.613614  0.241128  0.241406   \n",
       "3 -0.043875 -0.113002  -0.061584  ... -0.539735 -0.582077 -0.979074 -0.978556   \n",
       "4 -0.043875 -0.029140   0.242712  ... -0.577099 -0.600999  0.241128  0.241406   \n",
       "\n",
       "       f159      f160      f161      f162      f163      f164  \n",
       "0  1.072793  0.085530 -0.131155  0.677799 -0.120613 -0.119792  \n",
       "1  0.604120  0.008632 -0.131155  0.333211 -0.120613 -0.119792  \n",
       "2  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "3  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "4  0.018279 -0.068266 -0.084674 -0.054450 -1.760926 -1.760984  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append the class to the featuresDF based on txID\n",
    "featuresDF = featuresDF.merge(classesDF, on='txId')\n",
    "unlabled_featuresDF = unlabled_featuresDF.merge(unlabled_classesDF, on='txId')\n",
    "# Move features 'class' to first column\n",
    "cols = list(featuresDF.columns)\n",
    "cols = cols[:1] + [cols[-1]] + cols[1:-1]\n",
    "featuresDF = featuresDF[cols]\n",
    "\n",
    "featuresDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>timestep</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230460314</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230459870</td>\n",
       "      <td>1</td>\n",
       "      <td>0.961040</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.303743</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.480381</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.422589</td>\n",
       "      <td>-0.226790</td>\n",
       "      <td>-0.117629</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.413931</td>\n",
       "      <td>1.149556</td>\n",
       "      <td>-0.696053</td>\n",
       "      <td>-0.695540</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230333930</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171264</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.161887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157199</th>\n",
       "      <td>157581340</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.172974</td>\n",
       "      <td>-0.156732</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647874</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>7.165536</td>\n",
       "      <td>1.085202</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>5.157442</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157200</th>\n",
       "      <td>173077460</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.145771</td>\n",
       "      <td>-0.163752</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.135803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157201</th>\n",
       "      <td>158577750</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.165920</td>\n",
       "      <td>-0.123607</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.156418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>-1.760926</td>\n",
       "      <td>-1.760984</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157202</th>\n",
       "      <td>158654197</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.172842</td>\n",
       "      <td>-0.176622</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411776</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157203</th>\n",
       "      <td>157597225</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.132276</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157204 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             txId  timestep        f0        f1        f2        f3        f4  \\\n",
       "0         5530458         1 -0.171484 -0.184668 -1.201369 -0.121970 -0.043875   \n",
       "1       232022460         1 -0.172107 -0.184668 -1.201369 -0.121970 -0.043875   \n",
       "2       230460314         1  1.011523 -0.081127 -1.201369  1.153668  0.333276   \n",
       "3       230459870         1  0.961040 -0.081127 -1.201369  1.303743  0.333276   \n",
       "4       230333930         1 -0.171264 -0.184668 -1.201369 -0.121970 -0.043875   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "157199  157581340        49 -0.172974 -0.156732  1.018602 -0.121970 -0.043875   \n",
       "157200  173077460        49 -0.145771 -0.163752  0.463609 -0.121970 -0.043875   \n",
       "157201  158577750        49 -0.165920 -0.123607  1.018602 -0.121970 -0.043875   \n",
       "157202  158654197        49 -0.172842 -0.176622  1.018602 -0.121970 -0.043875   \n",
       "157203  157597225        49 -0.012037 -0.132276  0.463609 -0.121970 -0.043875   \n",
       "\n",
       "              f5        f6        f7  ...      f156      f157      f158  \\\n",
       "0      -0.113002 -0.061584 -0.162112  ...  0.673103 -0.979074 -0.978556   \n",
       "1      -0.113002 -0.061584 -0.162749  ...  0.439728 -0.979074 -0.978556   \n",
       "2       1.312656 -0.061584 -0.163523  ... -0.400422  0.517257  0.579382   \n",
       "3       1.480381 -0.061584 -0.163577  ... -0.422589 -0.226790 -0.117629   \n",
       "4      -0.113002 -0.061584 -0.161887  ... -0.607306 -0.979074 -0.978556   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "157199 -0.113002 -0.061584 -0.163636  ...  0.647874  0.241128  0.241406   \n",
       "157200 -0.113002 -0.061584 -0.135803  ... -0.613614  0.241128  0.241406   \n",
       "157201 -0.113002 -0.061584 -0.156418  ...  0.010822  1.461330  1.461369   \n",
       "157202 -0.113002 -0.061584 -0.163501  ... -0.411776  1.461330  1.461369   \n",
       "157203 -0.113002 -0.061584  0.001027  ... -0.613614  0.241128  0.241406   \n",
       "\n",
       "            f159      f160      f161      f162      f163      f164  class  \n",
       "0       0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792     -1  \n",
       "1      -0.098889 -0.106715 -0.131155 -0.183671 -0.120613 -0.119792     -1  \n",
       "2       0.018279  0.277775  0.326394  1.293750  0.178136  0.179117     -1  \n",
       "3       0.018279  0.277775  0.413931  1.149556 -0.696053 -0.695540     -1  \n",
       "4       0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792     -1  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "157199  7.165536  1.085202 -0.131155  5.157442 -0.120613 -0.119792     -1  \n",
       "157200  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792     -1  \n",
       "157201 -0.098889 -0.087490 -0.084674 -0.140597 -1.760926 -1.760984     -1  \n",
       "157202 -0.098889 -0.087490 -0.084674 -0.140597  1.519700  1.521399     -1  \n",
       "157203 -0.098889 -0.087490 -0.084674 -0.140597  1.519700  1.521399     -1  \n",
       "\n",
       "[157204 rows x 168 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabled_featuresDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "      <th>timestep</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>...</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5530458</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947382</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232022460</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670883</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230460314</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511871</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230459870</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.961040</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.303743</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.480381</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.504702</td>\n",
       "      <td>-0.422589</td>\n",
       "      <td>-0.226790</td>\n",
       "      <td>-0.117629</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.413931</td>\n",
       "      <td>1.149556</td>\n",
       "      <td>-0.696053</td>\n",
       "      <td>-0.695540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230333930</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171264</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569626</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        txId  class  timestep        f0        f1        f2        f3  \\\n",
       "0    5530458     -1         1 -0.171484 -0.184668 -1.201369 -0.121970   \n",
       "1  232022460     -1         1 -0.172107 -0.184668 -1.201369 -0.121970   \n",
       "2  230460314     -1         1  1.011523 -0.081127 -1.201369  1.153668   \n",
       "3  230459870     -1         1  0.961040 -0.081127 -1.201369  1.303743   \n",
       "4  230333930     -1         1 -0.171264 -0.184668 -1.201369 -0.121970   \n",
       "\n",
       "         f4        f5        f6  ...      f155      f156      f157      f158  \\\n",
       "0 -0.043875 -0.113002 -0.061584  ...  0.947382  0.673103 -0.979074 -0.978556   \n",
       "1 -0.043875 -0.113002 -0.061584  ...  0.670883  0.439728 -0.979074 -0.978556   \n",
       "2  0.333276  1.312656 -0.061584  ... -0.511871 -0.400422  0.517257  0.579382   \n",
       "3  0.333276  1.480381 -0.061584  ... -0.504702 -0.422589 -0.226790 -0.117629   \n",
       "4 -0.043875 -0.113002 -0.061584  ... -0.569626 -0.607306 -0.979074 -0.978556   \n",
       "\n",
       "       f159      f160      f161      f162      f163      f164  \n",
       "0  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "1 -0.098889 -0.106715 -0.131155 -0.183671 -0.120613 -0.119792  \n",
       "2  0.018279  0.277775  0.326394  1.293750  0.178136  0.179117  \n",
       "3  0.018279  0.277775  0.413931  1.149556 -0.696053 -0.695540  \n",
       "4  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move features 'class' to first column\n",
    "unlabled_cols = list(unlabled_featuresDF.columns)\n",
    "unlabled_cols = unlabled_cols[:1] + [unlabled_cols[-1]] + unlabled_cols[1:-1]\n",
    "unlabled_featuresDF = unlabled_featuresDF[cols]\n",
    "\n",
    "unlabled_featuresDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "      <th>timestep</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>...</th>\n",
       "      <th>f155</th>\n",
       "      <th>f156</th>\n",
       "      <th>f157</th>\n",
       "      <th>f158</th>\n",
       "      <th>f159</th>\n",
       "      <th>f160</th>\n",
       "      <th>f161</th>\n",
       "      <th>f162</th>\n",
       "      <th>f163</th>\n",
       "      <th>f164</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>232470704</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172900</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569626</td>\n",
       "      <td>-0.582077</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>232033533</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>230473487</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172290</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539735</td>\n",
       "      <td>-0.569462</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7089694</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.157608</td>\n",
       "      <td>-0.181768</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>0.328255</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>0.390171</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087993</td>\n",
       "      <td>0.036052</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>1.642711</td>\n",
       "      <td>4.052141</td>\n",
       "      <td>3.779090</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>231179595</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172729</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.024025</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.093204</td>\n",
       "      <td>-0.126239</td>\n",
       "      <td>1.299939</td>\n",
       "      <td>1.301521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>231177927</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171669</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.135448</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.011377</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2758467</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092328</td>\n",
       "      <td>1.239009</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>8.207194</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>8.608671</td>\n",
       "      <td>1.155601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.252616</td>\n",
       "      <td>-0.049041</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.074770</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>232437171</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.167243</td>\n",
       "      <td>-0.115037</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>0.403293</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>0.474034</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.517316</td>\n",
       "      <td>-0.537925</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3878886</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172886</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569626</td>\n",
       "      <td>-0.594691</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>231182296</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172310</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.024025</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.093204</td>\n",
       "      <td>-0.040092</td>\n",
       "      <td>1.299939</td>\n",
       "      <td>1.301521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14660781</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172771</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.024025</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.068266</td>\n",
       "      <td>-0.093204</td>\n",
       "      <td>-0.040092</td>\n",
       "      <td>1.299939</td>\n",
       "      <td>1.301521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>43358239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>230528714</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.030550</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.427640</td>\n",
       "      <td>-0.487465</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>230585122</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169339</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.569626</td>\n",
       "      <td>-0.607306</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2881274</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080015</td>\n",
       "      <td>1.124131</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>7.306744</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>5.757355</td>\n",
       "      <td>0.851305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887598</td>\n",
       "      <td>0.660488</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         txId  class  timestep        f0        f1        f2        f3  \\\n",
       "10  232470704      0         1 -0.172900 -0.184668 -1.201369  0.028105   \n",
       "11  232033533      0         1 -0.156114 -0.184668 -1.201369 -0.046932   \n",
       "12  230473487      0         1 -0.172290 -0.184668 -1.201369 -0.046932   \n",
       "13    7089694      0         1 -0.157608 -0.181768  0.463609  0.328255   \n",
       "14  231179595      0         1 -0.172729 -0.184668 -1.201369 -0.046932   \n",
       "15  231177927      0         1 -0.171669 -0.184668 -1.201369  0.028105   \n",
       "16    2758467      0         1  0.092328  1.239009 -0.646376  8.207194   \n",
       "17  232437171      0         1 -0.167243 -0.115037 -0.091383  0.403293   \n",
       "18    3878886      0         1 -0.172886 -0.184668 -1.201369 -0.046932   \n",
       "19  231182296      0         1 -0.172310 -0.184668 -1.201369 -0.046932   \n",
       "20   14660781      0         1 -0.172771 -0.184668 -1.201369 -0.046932   \n",
       "21   43358239      0         1 -0.029886 -0.184668 -1.201369 -0.121970   \n",
       "22  230528714      0         1 -0.030550 -0.184668 -1.201369 -0.121970   \n",
       "23  230585122      0         1 -0.169339 -0.081127 -1.201369 -0.121970   \n",
       "24    2881274      0         1  0.080015  1.124131  1.018602  7.306744   \n",
       "\n",
       "          f4        f5        f6  ...      f155      f156      f157      f158  \\\n",
       "10 -0.043875  0.054722 -0.061584  ... -0.569626 -0.582077  1.461330  1.461369   \n",
       "11 -0.043875 -0.029140 -0.061584  ... -0.577099 -0.600999  0.241128  0.241406   \n",
       "12 -0.043875 -0.029140 -0.061584  ... -0.539735 -0.569462  1.461330  1.461369   \n",
       "13 -0.043875  0.390171 -0.061584  ...  0.087993  0.036052  1.461330  1.461369   \n",
       "14 -0.024025 -0.029140 -0.061584  ... -0.577099 -0.600999  0.241128  0.241406   \n",
       "15 -0.043875  0.054722 -0.061584  ... -0.577099 -0.600999  0.241128  0.241406   \n",
       "16 -0.063725  8.608671  1.155601  ... -0.577099 -0.613614  0.241128  0.241406   \n",
       "17 -0.043875  0.474034 -0.061584  ... -0.517316 -0.537925 -0.979074 -0.978556   \n",
       "18 -0.043875 -0.029140 -0.061584  ... -0.569626 -0.594691 -0.979074 -0.978556   \n",
       "19 -0.024025 -0.029140 -0.061584  ... -0.577099 -0.600999  0.241128  0.241406   \n",
       "20 -0.024025 -0.029140 -0.061584  ... -0.577099 -0.600999  0.241128  0.241406   \n",
       "21 -0.043875 -0.113002 -0.061584  ... -0.577099 -0.613614  0.241128  0.241406   \n",
       "22 -0.043875 -0.113002 -0.061584  ... -0.427640 -0.487465 -0.979074 -0.978556   \n",
       "23 -0.043875 -0.113002 -0.061584  ... -0.569626 -0.607306 -0.979074 -0.978556   \n",
       "24 -0.043875  5.757355  0.851305  ...  0.887598  0.660488  1.461330  1.461369   \n",
       "\n",
       "        f159      f160      f161      f162      f163      f164  \n",
       "10  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "11 -0.098889 -0.087490 -0.084674 -0.140597  1.519700  1.521399  \n",
       "12  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "13  0.018279  1.642711  4.052141  3.779090  1.519700  1.521399  \n",
       "14 -0.098889 -0.087490 -0.093204 -0.126239  1.299939  1.301521  \n",
       "15  0.135448 -0.068266 -0.131155 -0.011377 -0.120613 -0.119792  \n",
       "16  0.252616 -0.049041 -0.131155  0.074770 -0.120613 -0.119792  \n",
       "17  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "18  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "19  0.018279 -0.068266 -0.093204 -0.040092  1.299939  1.301521  \n",
       "20  0.018279 -0.068266 -0.093204 -0.040092  1.299939  1.301521  \n",
       "21  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "22  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "23  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "24  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  \n",
       "\n",
       "[15 rows x 168 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresDF[10:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Create the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torcheval #https://pytorch.org/torcheval/stable/\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "from sklearn.metrics import ConfusionMatrixDisplay#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X_train,y_train):\n",
    "        \n",
    "        self.labels = y_train\n",
    "        self.features = X_train\n",
    "        \n",
    "        #self.vectors = self.data[self.features]\n",
    "        #self.vectors = torch.tensor(self.data[self.features].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        #print(self.data.shape)\n",
    "        #print(self.labels)\n",
    "        #print(self.features.shape)\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = torch.tensor(self.features[idx]).to(torch.float32).to(device)#.values\n",
    "        y = torch.tensor(self.labels[idx]).to(torch.float32).to(device)#.values\n",
    "        \n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = len(featuresDF)\n",
    "percentage_train = 0.7\n",
    "percentage_test = 0.3\n",
    "cut_train = int(total_length*percentage_train)\n",
    "X_train = np.array(featuresDF[:cut_train].drop('class', axis=1))\n",
    "y_train = np.array(featuresDF[:cut_train]['class'])\n",
    "X_test = np.array(featuresDF[cut_train:].drop('class', axis=1))\n",
    "y_test = np.array(featuresDF[cut_train:]['class'])\n",
    "# Create an instance of the custom dataset\n",
    "train_dataset = Data(X_train,y_train)\n",
    "test_dataset = Data(X_test, y_test)\n",
    "#[print(i.shape) for i in train_dataset]\n",
    "#Instance of the custom validation dataset\n",
    "#test_dataset = Data(os.path.join(dataset_root, \"val\"), transform=transform)\n",
    "\n",
    "# Define the batch size for the DataLoader\n",
    "batch_size = 3000\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the DataLoader for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If this cell fails you need to change the runtime of your colab notebook to GPU\n",
    "# Go to Runtime -> Change Runtime Type and select GPU\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157204, 167)\n"
     ]
    }
   ],
   "source": [
    "UX_train = np.array(unlabled_featuresDF.drop('class', axis=1))\n",
    "Uy_train = np.array(unlabled_featuresDF['class'])\n",
    "print(UX_train.shape)\n",
    "# Create an instance of the custom dataset\n",
    "unlabled_train_dataset = Data(UX_train,Uy_train)\n",
    "\n",
    "#Instance of the custom validation dataset\n",
    "#test_dataset = Data(os.path.join(dataset_root, \"val\"), transform=transform)\n",
    "\n",
    "# Define the batch size for the DataLoader\n",
    "batch_size = 3000\n",
    "\n",
    "# Create the DataLoader\n",
    "unlabled_train_loader = DataLoader(unlabled_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.32013274e+08,  1.00000000e+00, -1.23126716e-01, -1.84667551e-01,\n",
       "       -1.20136880e+00, -1.21969600e-01, -4.38745479e-02, -1.13002009e-01,\n",
       "       -6.15837941e-02, -1.12635219e-01, -1.19164648e-01, -4.97069644e-02,\n",
       "       -1.14995790e-01, -2.87412859e-02, -3.53905526e-02, -4.29552993e-02,\n",
       "       -1.32816149e-02,  6.28865779e-02, -1.28448427e-01, -1.33662990e-01,\n",
       "       -1.17240089e-01,  8.87057872e-01,  8.84556525e-01, -1.39710213e-01,\n",
       "       -1.48898494e-01, -8.01472697e-02, -1.55643640e-01, -1.07630095e-02,\n",
       "       -1.21074518e-02, -1.39712010e-01, -1.48893792e-01, -8.01467358e-02,\n",
       "       -1.55643608e-01, -1.06685611e-02, -1.20051821e-02, -2.46688307e-02,\n",
       "       -3.12723905e-02, -2.30451564e-02, -2.62146552e-02,  1.42781371e-03,\n",
       "        1.48264379e-03, -2.27215446e-01, -2.39368369e-01, -7.52555315e-02,\n",
       "       -2.34951518e-01,  3.74680288e-02,  4.34442213e-02, -2.27203324e-01,\n",
       "       -2.43236093e-01, -9.78946779e-02, -2.35896423e-01,  3.65766504e-02,\n",
       "        4.23451348e-02, -4.13940182e-01, -4.88286126e-01, -2.32552621e-01,\n",
       "       -4.67491762e-01,  4.87668179e-02,  5.29560645e-02, -3.90791082e-02,\n",
       "       -1.72871360e-01, -1.63111576e-01, -1.60898335e-01, -1.31634228e+00,\n",
       "       -1.31538839e+00, -3.90743779e-02, -1.72860318e-01, -1.63100239e-01,\n",
       "       -1.60891448e-01, -1.31633338e+00, -1.31537538e+00, -1.70316759e-02,\n",
       "       -3.00262353e-02, -1.76401196e-02, -1.50709000e-02, -1.40763222e-01,\n",
       "       -1.40334624e-01, -9.54026893e-02, -2.63373037e-01, -2.49491858e-01,\n",
       "       -2.62691689e-01, -1.47176449e+00, -1.47027746e+00, -5.90130548e-02,\n",
       "       -2.61295119e-01, -2.54008492e-01, -2.58101539e-01, -1.49997154e+00,\n",
       "       -1.49858528e+00, -2.93719543e-01,  2.98105207e-01,  5.87701105e-01,\n",
       "        2.24544335e-01, -1.08490679e+00, -1.08484490e+00, -1.24872367e-01,\n",
       "       -1.66815470e-01, -1.16816715e-01, -1.48135908e-01, -1.46587757e-02,\n",
       "       -1.88487130e-02, -1.45817740e+00, -1.49428481e+00, -8.34587840e-02,\n",
       "       -1.48619933e+00, -8.87977254e-02, -9.04370819e-02, -1.66549949e-01,\n",
       "       -2.16536379e-01, -1.34546475e-01, -1.94816611e-01, -3.17499022e-03,\n",
       "       -4.09409036e-03, -1.09633569e+00, -1.26733991e+00, -3.49932775e-01,\n",
       "       -1.23044080e+00, -4.35761404e-03, -4.19437024e-03, -1.16424604e-01,\n",
       "       -1.76617282e-01, -1.37323277e-01, -1.52464371e-01, -2.60596888e-02,\n",
       "       -2.76597140e-02, -9.31447270e-02, -1.43706724e-01, -9.77185991e-02,\n",
       "       -1.27462250e-01,  3.14329643e-03,  2.42630989e-03, -1.55519863e-02,\n",
       "       -1.21019079e-01, -1.12832733e-01, -1.01401414e-01, -1.15964880e+00,\n",
       "       -1.16012886e+00, -1.37393228e+00, -1.35628882e+00, -3.01685553e-01,\n",
       "       -1.40463583e+00,  1.34200300e+00,  1.34073273e+00, -1.68741553e-01,\n",
       "       -3.63728568e-01, -3.09201862e-01, -3.48362976e-01, -1.01596337e+00,\n",
       "       -1.01623043e+00, -9.68902875e-01, -8.98426267e-01,  1.53209334e-01,\n",
       "       -1.07188529e+00, -1.11691817e+00, -1.11694847e+00, -2.16814361e-01,\n",
       "       -2.49619286e-01, -1.28722496e-01, -2.35167555e-01, -9.79073767e-01,\n",
       "       -9.78556047e-01, -9.88887367e-02, -8.74901561e-02, -8.46742330e-02,\n",
       "       -1.40597079e-01,  1.51969962e+00,  1.52139948e+00])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UX_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=167, out_features=50),\n",
    "            #nn.Dropout(p=0.3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.LayerNorm(50),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features = 50, out_features  = 1),\n",
    "            nn.Sigmoid()\n",
    "            #12x12x16\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, loader, optimizer, criterion, reshape=False):\n",
    "    loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch_features, labels in loader:\n",
    "        # load it to the active device\n",
    "        batch_features = batch_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # reset the gradients back to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute predictions\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        # compute training loss\n",
    "        train_loss = criterion(outputs, labels)\n",
    "\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "\n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(loader)\n",
    "    print(\"epoch : {}/{}, Train loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "\n",
    "def model_test(model, test_loader, criterion, reshape=False):\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    tot_pred_labels = torch.tensor([]).to(device)\n",
    "    tot_true_labels = torch.tensor([]).to(device)\n",
    "\n",
    "    for features, labels in test_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "        print(\"outputs\", outputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        \n",
    "        #outputs = torch.nn.functional.sigmoid(outputs)\n",
    "        #pred_labels = torch.argmax(outputs, dim = 1)\n",
    "        \n",
    "        pred_labels = torch.round(outputs)\n",
    "        #print(\"outputs\",outputs, outputs.shape)\n",
    "        #print(\"labels\",pred_labels)\n",
    "        test_acc = torch.sum(pred_labels == labels)/len(pred_labels)\n",
    "        print(\"TEST ACCURACY:\", test_acc)\n",
    "        true_labels = labels\n",
    "        tot_pred_labels = torch.cat((tot_pred_labels, pred_labels), dim = 0)\n",
    "        tot_true_labels = torch.cat((tot_true_labels, true_labels), dim = 0)\n",
    "        # compute training loss\n",
    "        \n",
    "        test_loss = criterion(outputs,labels)\n",
    "\n",
    "        # add the batch training loss to epoch loss\n",
    "        loss += test_loss.item()\n",
    "\n",
    "    # compute the epoch test loss\n",
    "    \n",
    "    loss = loss / len(test_loader)\n",
    "\n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, Test loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    return tot_pred_labels,tot_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_samples(model, unlabled_loader,threshold):\n",
    "    added_samples = []\n",
    "    non_added_samples = []\n",
    "    true_added_labels = torch.tensor([]).to(device)\n",
    "    pred_added_labels = torch.tensor([]).to(device)\n",
    "    tot_pred_labels = torch.tensor([]).to(device)\n",
    "    tot_true_labels = torch.tensor([]).to(device)\n",
    "    #new_data = labled_data\n",
    "\n",
    "    for img, lab in unlabled_loader:\n",
    "        img = img.to(device)\n",
    "        lab = lab.to(device)\n",
    "        with torch.no_grad():\n",
    "            self_outputs = model(img)\n",
    "            \n",
    "            self_outputs = self_outputs.squeeze()\n",
    "            \n",
    "\n",
    "        #soft_outputs = torch.nn.functional.sigmoid(self_outputs)\n",
    "        \n",
    "        #probs,pred_labels = torch.max(soft_outputs, dim=0)\n",
    "        #print(\"probs:\",probs.shape, probs)\n",
    "        for idx,prob in enumerate(self_outputs):\n",
    "            if prob.item() > threshold:\n",
    "\n",
    "                added_samples.append((img[idx], torch.round(prob).item()))\n",
    "                #print(\"added_samp:\", added_samples[0][0],added_samples[0][1],added_samples)\n",
    "                true_added_labels = torch.cat((true_added_labels, lab[idx].reshape(1)), dim = 0)\n",
    "                pred_added_labels = torch.cat((pred_added_labels, torch.round(prob).reshape(1)), dim = 0)\n",
    "                \n",
    "            elif prob.item() < (1-threshold):\n",
    "                added_samples.append((img[idx], torch.round(prob).item()))\n",
    "                #print(\"added_samp:\", added_samples[0][0],added_samples[0][1],added_samples)\n",
    "                true_added_labels = torch.cat((true_added_labels, lab[idx].reshape(1)), dim = 0)\n",
    "                pred_added_labels = torch.cat((pred_added_labels, torch.round(prob).reshape(1)), dim = 0)\n",
    "            else:\n",
    "                non_added_samples.append((img[idx], -1))\n",
    "\n",
    "        true_labels = lab\n",
    "        tot_pred_labels = torch.cat((tot_pred_labels, torch.round(prob).unsqueeze(0)), dim = 0)\n",
    "        tot_true_labels = torch.cat((tot_true_labels, true_labels), dim = 0)\n",
    "\n",
    "\n",
    "    num_correct_preds = torch.sum(torch.eq(true_added_labels,pred_added_labels)).item()\n",
    "\n",
    "    print(f'number of items added to the labelled data: {len(added_samples)}')\n",
    "    print(f'correctly classified items added to the labelled data: {num_correct_preds}')\n",
    "    print(f'incorrectly classified items added to the labelled data: {len(added_samples) - num_correct_preds}')\n",
    "    \n",
    "    return added_samples, non_added_samples #new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_train(model, X, y,UX_train,Uy_train,threshold, optimizer, criterion, reshape=False):\n",
    "    loss = 0\n",
    "    model.train()\n",
    "    train_dataset = Data(X,y)\n",
    "    labled_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    if len(UX_train) != 0:\n",
    "        unlabled_dataset = Data(UX_train, Uy_train)\n",
    "        unlabled_loader = torch.utils.data.DataLoader(unlabled_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f'labled loader length: {len(labled_loader)}')\n",
    "    \n",
    "    for batch_features,labels in labled_loader:\n",
    "        \n",
    "        # load it to the active device\n",
    "        \n",
    "        batch_features = batch_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # reset the gradients back to zero\n",
    "        optimizer.zero_grad()\n",
    "        # compute reconstructions\n",
    "        outputs = model(batch_features)\n",
    "        outputs = outputs.squeeze()\n",
    "        #print(outputs)\n",
    "        #outputs = torch.nn.functional.sigmoid(outputs.squeeze())\n",
    "        \n",
    "        #print(outputs.dtype, labels.dtype)\n",
    "        # compute training loss\n",
    "        #print(\"outputs:\",outputs.shape)\n",
    "        labels = labels.to(torch.float32)\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        pred_labels = torch.round(outputs)\n",
    "        train_acc = torch.sum(pred_labels == labels)/len(pred_labels)\n",
    "        print(\"TRAIN ACCURACY:\", train_acc)\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "        \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(labled_loader)\n",
    "    print(\"epoch : {}/{}, Train loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "    \n",
    "    if len(UX_train) != 0:\n",
    "        print(\"ux_train\")\n",
    "        add_data, non_add_data = add_samples(model, unlabled_loader,threshold)\n",
    "        if len(add_data) != 0:\n",
    "            X_unlabled= [np.array(XU.cpu()) for XU,yU in add_data]\n",
    "            y_unlabled = [np.array(yU) for XU,yU in add_data]\n",
    "            #new_data = labled_data + add_data\n",
    "            new_X = np.concatenate((X, np.array(X_unlabled)), axis = 0)\n",
    "            new_y = np.concatenate((y,np.array(y_unlabled)), axis = 0)\n",
    "        \n",
    "            X_non_add= [np.array(XU_non.cpu()) for XU_non,yU_non in non_add_data]\n",
    "            y_non_add = [np.array(yU_non) for XU_non,yU_non in non_add_data]\n",
    "        \n",
    "            return new_X, new_y, X_non_add, y_non_add\n",
    "        else:\n",
    "            return X, y,UX_train, Uy_train\n",
    "    else: \n",
    "        return X, y, [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(featuresDF[:cut_train].drop('class', axis=1))\n",
    "y_train = np.array(featuresDF[:cut_train]['class'])\n",
    "X_test = np.array(featuresDF[cut_train::].drop('class', axis=1))\n",
    "y_test = np.array(featuresDF[cut_train::]['class'])\n",
    "# Create an instance of the custom dataset\n",
    "train_dataset = Data(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157204, 167)\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "UX_train = np.array(unlabled_featuresDF.drop('class', axis=1))\n",
    "Uy_train = np.array(unlabled_featuresDF['class'])\n",
    "print(UX_train.shape)\n",
    "print(np.unique(Uy_train))\n",
    "# Create an instance of the custom dataset\n",
    "unlabled_train_dataset = Data(UX_train,Uy_train)\n",
    "\n",
    "#Instance of the custom validation dataset\n",
    "#test_dataset = Data(os.path.join(dataset_root, \"val\"), transform=transform)\n",
    "\n",
    "# Define the batch size for the DataLoader\n",
    "batch_size = 3000\n",
    "\n",
    "# Create the DataLoader\n",
    "unlabled_train_loader = DataLoader(unlabled_train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Data(X_test, y_test)\n",
    "# Create the DataLoader for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labled loader length: 11\n",
      "TRAIN ACCURACY: tensor(0.8207, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8267, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8323, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8360, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8420, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8403, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8300, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8323, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8420, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8447, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8524, device='cuda:0')\n",
      "epoch : 1/10000, Train loss = 0.558505\n",
      "ux_train\n",
      "number of items added to the labelled data: 0\n",
      "correctly classified items added to the labelled data: 0\n",
      "incorrectly classified items added to the labelled data: 0\n",
      "outputs tensor([[0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        ...,\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854]], device='cuda:0')\n",
      "TEST ACCURACY: tensor(0.9427, device='cuda:0')\n",
      "outputs tensor([[0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        ...,\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854]], device='cuda:0')\n",
      "TEST ACCURACY: tensor(0.9307, device='cuda:0')\n",
      "outputs tensor([[0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        ...,\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854]], device='cuda:0')\n",
      "TEST ACCURACY: tensor(0.9387, device='cuda:0')\n",
      "outputs tensor([[0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        ...,\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854]], device='cuda:0')\n",
      "TEST ACCURACY: tensor(0.9377, device='cuda:0')\n",
      "outputs tensor([[0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        ...,\n",
      "        [0.3854],\n",
      "        [0.3854],\n",
      "        [0.3854]], device='cuda:0')\n",
      "TEST ACCURACY: tensor(0.9350, device='cuda:0')\n",
      "epoch : 1/10000, Test loss = 0.516262\n",
      "labled loader length: 11\n",
      "TRAIN ACCURACY: tensor(0.8373, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8330, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8473, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8517, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8517, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8470, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8617, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8567, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8620, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8630, device='cuda:0')\n",
      "TRAIN ACCURACY: tensor(0.8620, device='cuda:0')\n",
      "epoch : 2/10000, Train loss = 0.541626\n",
      "ux_train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 5\u001b[0m     X_train, y_train, UX_train, Uy_train \u001b[38;5;241m=\u001b[39m self_train(model, X_train, y_train,UX_train,Uy_train ,threshold, optimizer, criterion, reshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     tot_pred_labels,tot_true_labels \u001b[38;5;241m=\u001b[39m model_test(model, test_loader, criterion, reshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m num_correct_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39meq(tot_true_labels,tot_pred_labels))\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[137], line 50\u001b[0m, in \u001b[0;36mself_train\u001b[0;34m(model, X, y, UX_train, Uy_train, threshold, optimizer, criterion, reshape)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(UX_train) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mux_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     add_data, non_add_data \u001b[38;5;241m=\u001b[39m add_samples(model, unlabled_loader,threshold)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(add_data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     52\u001b[0m         X_unlabled\u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(XU\u001b[38;5;241m.\u001b[39mcpu()) \u001b[38;5;28;01mfor\u001b[39;00m XU,yU \u001b[38;5;129;01min\u001b[39;00m add_data]\n",
      "Cell \u001b[0;32mIn[136], line 24\u001b[0m, in \u001b[0;36madd_samples\u001b[0;34m(model, unlabled_loader, threshold)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#soft_outputs = torch.nn.functional.sigmoid(self_outputs)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#probs,pred_labels = torch.max(soft_outputs, dim=0)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(\"probs:\",probs.shape, probs)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(self_outputs):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m threshold:\n\u001b[1;32m     26\u001b[0m         added_samples\u001b[38;5;241m.\u001b[39mappend((img[idx], torch\u001b[38;5;241m.\u001b[39mround(prob)\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m#print(\"added_samp:\", added_samples[0][0],added_samples[0][1],added_samples)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "threshold = 0.99\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    X_train, y_train, UX_train, Uy_train = self_train(model, X_train, y_train,UX_train,Uy_train ,threshold, optimizer, criterion, reshape=True)\n",
    "    tot_pred_labels,tot_true_labels = model_test(model, test_loader, criterion, reshape=True)\n",
    "\n",
    "num_correct_preds = torch.sum(torch.eq(tot_true_labels,tot_pred_labels)).item()\n",
    "accuracy = num_correct_preds/len(tot_true_labels)\n",
    "f1_score = multiclass_f1_score(tot_pred_labels,tot_true_labels)\n",
    "confusion = confusion_matrix(tot_true_labels.cpu(),tot_pred_labels.cpu() )\n",
    "avg_score = (accuracy + f1_score)/2\n",
    "#best_score = actualize_best_results(best_score,(avg_score.item(), threshold))\n",
    "\n",
    "print(f'Accuracy : {accuracy} threshold: {threshold}' )\n",
    "print(f'F1 score: {f1_score.cpu().item()} threshold {threshold}')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs=5\n",
    "for epoch in range(epochs):\n",
    "    model_train(model, train_loader, optimizer, criterion, reshape=True)\n",
    "    tot_pred_labels,tot_true_labels = model_test(model, val_loader, criterion, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.round(torch.tensor([0.55, 0.7,0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into the dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "licitSamples = featuresDF[featuresDF['class'] == 0]\n",
    "illicitSamples = featuresDF[featuresDF['class'] == 1]\n",
    "\n",
    "# Obtain the needed samples to balance the dataset\n",
    "AE_data = licitSamples.sample(n=len(licitSamples) - len(illicitSamples), random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(AE_data.iloc[:, 2:])\n",
    "AE_data.iloc[:, 2:] = scaler.transform(AE_data.iloc[:, 2:])\n",
    "\n",
    "# Store licitSamples in a csv file\n",
    "#AE_data.to_csv('../../../data/noPCA_AEData2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainAE, valAE = train_test_split(AE_data, test_size=0.15, random_state=42)\n",
    "\n",
    "train_dataset = Data(trainAE)\n",
    "val_dataset = Data(valAE)\n",
    "\n",
    "print(f'Train size: {len(train_dataset)}')\n",
    "print(f'Validation size: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets into Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Create the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create autoencoder for input vectors of size 65\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(166, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 86),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(86, 48),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(48, 86),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(86, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 166),\n",
    "            nn.LeakyReLU() \n",
    "        )\n",
    "    \n",
    "    def forward(self, in_features):\n",
    "        encoded_features = self.encoder(in_features)\n",
    "        out = self.decoder(encoded_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Create the train and validate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_features in train_loader:\n",
    "        batch_features = batch_features.view(-1, 166).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_features)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, val_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_features in val_loader:\n",
    "            batch_features = batch_features.view(-1, 166).to(device)\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_features)\n",
    "            val_loss += loss.item()*10\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to do both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs, device='cuda', name='AE_loss'):\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train(model, criterion, optimizer, train_loader, device)\n",
    "        val_loss = validate(model, criterion, val_loader, device)\n",
    "        scheduler.step()\n",
    "        if epoch%100==0:\n",
    "            print(f'Epoch: {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}\\n')\n",
    "            losses.append([train_loss, val_loss])\n",
    "    # plot the evolving loss\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot([i[0] for i in losses], label='Training Loss')\n",
    "    plt.plot([i[1] for i in losses], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/' + str(name) + '.png')\n",
    "    plt.show()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Train small sample of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "losses = train_and_val(model, criterion, optimizer, scheduler, train_loader, val_loader, epochs=3500, device=device, name='noPCA_Autoencoder_secondRUN')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'models/NoPCA_Autoencoder_secondRUN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch12",
   "language": "python",
   "name": "pytorch12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
